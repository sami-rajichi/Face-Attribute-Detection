{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":367971,"sourceType":"datasetVersion","datasetId":160647},{"sourceId":900167,"sourceType":"datasetVersion","datasetId":481889},{"sourceId":6846122,"sourceType":"datasetVersion","datasetId":3935781},{"sourceId":7076547,"sourceType":"datasetVersion","datasetId":4076079}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Gender Prediction using CNNs:<br>\n    -- An Exploration of Image Recognition with InceptionV3 and Xception Models --</h1></center>\n\n<p>\n  <h2>Introduction:</h2><br>\n  Image recognition stands as a pivotal application in the realm of Machine Learning, offering solutions for a diverse range of challenges such as security, object detection, face recognition, healthcare, and entertainment. Its potential to contribute significantly to societal well-being underscores the importance of exploring new applications, refining existing methodologies, and extracting more precise and valuable insights. An illustrative example of the impactful applications of image recognition is evident in the research conducted by The Chinese University of Hong Kong, where deep learning techniques were employed for face detection (<a href=\"https://arxiv.org/abs/1509.06451\" target=\"_blank\">source</a>).<br><br>\n\n  In this project, we aim to construct a Machine Learning Algorithm utilizing Convolutional Neural Networks (CNNs) through pre-trained models, including Inception V3 and Xception. The primary objective is to predict the gender (male or female) of an individual based on input images, showcasing the versatility and efficacy of image classification techniques.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>Dataset Overview:</h2>\n\n<p>This project utilizes the <strong>men-women-classification</strong> dataset, accessible through Kaggle, and specifically focuses on the Men/Women Classification Dataset. The dataset is widely recognized in computer vision and deep learning circles, playing a crucial role in tasks such as face detection. It serves as an excellent resource for training and testing models designed to recognize facial attributes, including features like hair color, smiles, or the presence of glasses. The dataset encompasses a broad spectrum of challenges, including diverse poses, background variations, and a rich diversity of individuals. It comprises a substantial quantity of images accompanied by comprehensive annotations.</p>\n\n<h3>Men / Women Classification Dataset:</h3>\n\n<p>This manually curated and meticulously cleaned (<a href=\"https://www.kaggle.com/datasets/playlist/men-women-classification/data\" target=\"_blank\">men-women-classification</a>) dataset consists of 3,354 images (in JPG format), categorized into men (1,414 files) and women (1,940 files). The dataset aims to facilitate the development and evaluation of models focused on gender classification. Each image provides valuable insights into recognizing gender-related attributes, contributing to the broader field of face analysis.</p>\n\n<h3>Validation Dataset:</h3>\n\n<p>For validation purposes, a separate dataset is employed (<a href=\"https://www.kaggle.com/jessicali9530/celeba-dataset\" target=\"_blank\">celeba-dataset</a>), featuring 202,599 face images of numerous celebrities, spanning 10,177 unique identities. While the names of the identities are not disclosed, each image is annotated with 40 binary attributes and includes information about the locations of five facial landmarks. In the context of this project, the focus is specifically on utilizing the images from this dataset for validation purposes, omitting the additional attribute and landmark data.</p>","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"# === Essential Libraries ===\n# Importing necessary libraries for machine learning and visualization.\nimport keras\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport os\nimport pandas as pd\nimport random \nimport io\nimport cv2\nfrom IPython.display import HTML, display\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\n\n# === Image Data Handling ===\n# Utilizing modules for managing image data, including loading, converting, and augmenting.\nfrom keras.utils import image_dataset_from_directory, load_img, img_to_array, plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# === Model and Layer Components ===\n# Setting up components related to building neural network models.\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\n\n# === Model Evaluation ===\n# Utilizing modules for evaluation and testing metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, ConfusionMatrixDisplay\n\n# === Random Seed Setting ===\n# Ensuring reproducibility by setting random seeds.\ninitializer = glorot_uniform(seed=42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# === Explanation ===\n# These sections import libraries, handle image data, define model components,\n# optimize and regularize, and set random seeds for reproducibility.\n# Each serves a crucial role in building and training deep learning models for image classification tasks.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T19:07:32.688272Z","iopub.execute_input":"2023-12-10T19:07:32.688681Z","iopub.status.idle":"2023-12-10T19:07:32.699626Z","shell.execute_reply.started":"2023-12-10T19:07:32.688644Z","shell.execute_reply":"2023-12-10T19:07:32.698510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Variables","metadata":{}},{"cell_type":"code","source":"IMG_WIDTH = 178\nIMG_HEIGHT = 218\nBATCH_SIZE = 16\nNUM_EPOCHS = 20","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:35.314053Z","iopub.execute_input":"2023-12-10T19:07:35.314909Z","iopub.status.idle":"2023-12-10T19:07:35.320021Z","shell.execute_reply.started":"2023-12-10T19:07:35.314868Z","shell.execute_reply":"2023-12-10T19:07:35.318992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Functions for use","metadata":{}},{"cell_type":"code","source":"def load_random_images(directory, num_images_per_class, class_name=['men', 'women']):\n    \"\"\"\n    Load a specified number of random images from each class in a given directory using flow_from_directory.\n\n    Parameters:\n    - directory: Path to the directory containing the images.\n    - num_images_per_class: Number of random images to load from each class.\n    - Class_Name: List of class names.\n\n    Returns:\n    - Img: List of loaded images.\n    - Label: List of corresponding labels.\n    \"\"\"\n\n    # Creating an ImageDataGenerator for normalization.\n    data_generator = ImageDataGenerator(rescale=1./255)\n\n    # Lists to store loaded images and labels.\n    img = []\n    labels = []\n\n    for class_name in class_name:\n        class_directory = os.path.join(directory, class_name)\n\n        # Collecting a list of image file names for the current class.\n        image_file_names = os.listdir(class_directory)\n\n        # Selecting num_images_per_class random images from the current class.\n        selected_images = random.sample(image_file_names, num_images_per_class)\n\n        for image_name in selected_images:\n            image_path = os.path.join(class_directory, image_name)\n\n            # Loading and processing the image.\n            image = load_img(image_path, target_size=(IMG_WIDTH, IMG_HEIGHT))\n            image_array = img_to_array(image)\n            label = class_name\n\n            # Adding the loaded image and label to the lists.\n            img.append(image_array)\n            labels.append(label.title())\n\n    return img, labels","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:37.972531Z","iopub.execute_input":"2023-12-10T19:07:37.972925Z","iopub.status.idle":"2023-12-10T19:07:37.981524Z","shell.execute_reply.started":"2023-12-10T19:07:37.972891Z","shell.execute_reply":"2023-12-10T19:07:37.980077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images_subplots(directory):\n    \"\"\"\n    Plot images in a 4x4 grid of subplots using Plotly.\n\n    Parameters:\n    - Img: List of loaded images.\n    - Label: List of corresponding labels.\n\n    Returns:\n    - None\n    \"\"\"\n    # Grab images \n    num_images_per_class = 3\n    Img, Label = load_random_images(directory, num_images_per_class)\n    \n    # Create a 4x4 grid of subplots\n    fig = make_subplots(rows=2, cols=3, subplot_titles=[f\"{i}\" for i in Label], horizontal_spacing=0.02)\n\n    s = 0\n    for i in range(2):\n        for j in range(3):\n            # Add an image to each subplot\n            fig.add_trace(go.Image(z=Img[s], dx=224, dy=224), row=i+1, col=j+1)\n            s += 1\n\n    # Hide x-axis tick labels\n    fig.update_xaxes(showticklabels=False)\n    # Hide y-axis tick labels\n    fig.update_yaxes(showticklabels=False)\n    \n    # Update layout\n    fig.update_layout(\n        paper_bgcolor='#E7C18A',\n        title_text='Gender Wise Images',\n        title_x=0.5,  # Center the title horizontally\n        title_y=0.95,  # Center the title vertically\n        height=500\n    )\n\n    # Show the figure\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:39.005900Z","iopub.execute_input":"2023-12-10T19:07:39.006305Z","iopub.status.idle":"2023-12-10T19:07:39.016999Z","shell.execute_reply.started":"2023-12-10T19:07:39.006273Z","shell.execute_reply":"2023-12-10T19:07:39.016049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_custom_model(base_model):\n    \"\"\"\n    Build a custom model on top of a pre-trained base model for a specific task.\n\n    Parameters:\n    - base_model: The pre-trained base model.\n\n    Returns:\n    - model: The custom model.\n    \"\"\"\n\n    # Freeze the layers of the pre-trained model\n    for layer in base_model.layers[:52]:\n        layer.trainable = False\n\n    # Add custom layers for the specific task\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Flatten()(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation=\"relu\", kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2=0.005))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation=\"relu\", kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2=0.005))(x)\n    x = Dense(148, activation=\"relu\", kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2=0.005))(x)\n\n    output = Dense(2, activation='softmax')(x)\n\n    # Create the final model\n    model = Model(inputs=base_model.input, outputs=output)\n\n    # Compile the model with an appropriate optimizer and loss function\n    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:40.020900Z","iopub.execute_input":"2023-12-10T19:07:40.022008Z","iopub.status.idle":"2023-12-10T19:07:40.030802Z","shell.execute_reply.started":"2023-12-10T19:07:40.021955Z","shell.execute_reply":"2023-12-10T19:07:40.029600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_generator, valid_generator, save_model, epochs=NUM_EPOCHS):\n    \"\"\"\n    Train a given model using the provided data generators.\n\n    Parameters:\n    - model: The model to be trained.\n    - train_generator: Data generator for training data.\n    - valid_generator: Data generator for validation data.\n    - save_model: Path to save the best model checkpoint.\n    - epochs: Number of training epochs (default is NUM_EPOCHS).\n\n    Returns:\n    - history: Training history.\n    - model: Trained model.\n    \"\"\"\n\n    # Define callbacks for model training\n    checkpoint = ModelCheckpoint(save_model, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n    # early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n\n    # Train the model\n    history = model.fit(\n        train_generator,\n        validation_data=valid_generator,\n        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n        validation_steps=valid_generator.samples // valid_generator.batch_size,\n        epochs=epochs,\n        callbacks=[checkpoint],\n        verbose=1,\n    )\n\n    return history\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:41.225922Z","iopub.execute_input":"2023-12-10T19:07:41.226347Z","iopub.status.idle":"2023-12-10T19:07:41.233889Z","shell.execute_reply.started":"2023-12-10T19:07:41.226314Z","shell.execute_reply":"2023-12-10T19:07:41.233058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"\n    Plot the training history, including accuracy and loss, over epochs.\n\n    Parameters:\n    - history: The training history obtained from model training.\n\n    Returns:\n    None\n    \"\"\"\n\n    # Plot accuracy through epochs\n    plt.figure(figsize=(18, 4))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    plt.title('Accuracy')\n    plt.show()\n\n    # Plot loss function value through epochs\n    plt.figure(figsize=(18, 4))\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='valid')\n    plt.legend()\n    plt.title('Loss Function')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:42.526588Z","iopub.execute_input":"2023-12-10T19:07:42.527015Z","iopub.status.idle":"2023-12-10T19:07:42.533564Z","shell.execute_reply.started":"2023-12-10T19:07:42.526946Z","shell.execute_reply":"2023-12-10T19:07:42.532702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_to_display(filename):\n    \"\"\"\n    Convert an image file to a base64-encoded string for displaying inline.\n\n    Parameters:\n    - filename (str): Path to the image file.\n\n    Returns:\n    - str: Base64-encoded string representing the image.\n    \"\"\"\n    # Open the image using PIL\n    image = Image.open(filename)\n    \n    # Resize the image to a thumbnail\n    image.thumbnail((450, 450), Image.LANCZOS)\n\n    # Save the image to a buffer in JPEG format\n    with BytesIO() as buffer:\n        image.save(buffer, 'jpeg')\n        \n        # Encode the image buffer to base64\n        return base64.b64encode(buffer.getvalue()).decode()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:43.747172Z","iopub.execute_input":"2023-12-10T19:07:43.747792Z","iopub.status.idle":"2023-12-10T19:07:43.753387Z","shell.execute_reply.started":"2023-12-10T19:07:43.747759Z","shell.execute_reply":"2023-12-10T19:07:43.752294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_random_image_with_prediction(model, dataset_path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n    \"\"\"\n    Get the image array and prediction for a random image from a dataset.\n\n    Parameters:\n    - model (tf.keras.Model): The trained model for making predictions.\n    - dataset_path (str): Path to the directory containing the images.\n    - target_size (tuple): Size to resize the displayed image.\n\n    Returns:\n    - tuple: (image_array, prediction)\n    \"\"\"\n    images, predictions = [], []\n    for _ in range(3):\n        # Get a list of all image files in the dataset directory\n        image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n\n        if not image_files:\n            print(\"No image files found in the specified directory.\")\n            return None\n\n        # Select a random image file\n        random_image_file = random.choice(image_files)\n        random_image_path = os.path.join(dataset_path, random_image_file)\n\n        # Open the image using PIL\n        image = Image.open(random_image_path)\n\n        # Resize the image to the target size\n        image = image.resize(target_size)\n        # Convert the image to a NumPy array\n        image_array = img_to_array(image)\n        images.append(random_image_path)\n        image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n        image_array /= 255.0\n\n        # Make a prediction using the model\n        prediction = model.predict(image_array)[0]\n        argmax = np.argmax(prediction)\n        predictions.append(('Woman' if argmax == 1 else 'Man', prediction[argmax]))\n    # Return the image array and prediction\n    return images, predictions","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:45.118738Z","iopub.execute_input":"2023-12-10T19:07:45.119409Z","iopub.status.idle":"2023-12-10T19:07:45.127542Z","shell.execute_reply.started":"2023-12-10T19:07:45.119367Z","shell.execute_reply":"2023-12-10T19:07:45.126478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_images_with_predictions(images, predictions, card_width=300, border_radius=5):\n    \"\"\"\n    Display images along with their predicted class and probability in a customizable card display.\n\n    Parameters:\n    - images: List of image paths or PIL Image objects.\n    - predictions: List of tuples (class, probability) for each image.\n    - card_width: Width of the card in pixels.\n    - border_radius: Border radius for image corners in pixels.\n\n    Returns:\n    None\n    \"\"\"\n\n    # Custom CSS for card display\n    custom_css = f\"\"\"\n    <style>\n        .card {{\n            border: 1px solid #ddd;\n            padding: 10px;\n            margin: 10px;\n            text-align: center;\n            width: {card_width}px;\n            display: inline-block;\n            border-radius: {border_radius}px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\n            transition: box-shadow 0.3s ease-in-out;\n        }}\n\n        .card:hover {{\n            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2);\n        }}\n\n        .card img {{\n            width: 100%;\n            border-top-left-radius: {border_radius}px;\n            border-top-right-radius: {border_radius}px;\n        }}\n\n        .prediction-info {{\n            margin-top: 10px;\n        }}\n    </style>\n    \"\"\"\n\n    display(HTML(custom_css))\n\n    # Display images and predictions in cards\n    for img, prediction in zip(images, predictions):\n        # Display card with image, predicted class, and probability\n        card_html = f\"\"\"\n        <div class=\"card\">\n            <img src=\"data:image/jpeg;base64,{img_to_display(img)}\" alt=\"Image\" style=\"width:100%\">\n            <div class=\"prediction-info\">\n                <p style=\"font-weight: bold;\">Predicted Class: {prediction[0]}</p>\n                <p>Probability: {prediction[1]:.4f}</p>\n            </div>\n        </div>\n        \"\"\"\n\n        display(HTML(card_html))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:46.082143Z","iopub.execute_input":"2023-12-10T19:07:46.082555Z","iopub.status.idle":"2023-12-10T19:07:46.089572Z","shell.execute_reply.started":"2023-12-10T19:07:46.082522Z","shell.execute_reply":"2023-12-10T19:07:46.088462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_images_and_labels_for_testing(\n    directory='/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba', \n    csv_path='/kaggle/input/celeba-dataset/list_attr_celeba.csv',\n    target_size=(IMG_HEIGHT, IMG_WIDTH)):\n    \"\"\"\n    Load images and corresponding labels from a directory and CSV file.\n\n    Parameters:\n    - directory: Path to the directory containing the images.\n    - csv_path: Path to the CSV file with gender labels.\n\n    Returns:\n    - images: List of loaded images as numpy arrays.\n    - labels: List of corresponding gender labels.\n    \"\"\"\n    # Load CSV file\n    df = pd.read_csv(csv_path)\n\n    # Extract labels from the \"Male\" column\n    labels = df['Male'].values[:1000]\n    for l in range(len(labels)):\n        labels[l] = 1 if labels[l] == -1 else 0\n    \n    # Lists to store loaded images and labels\n    images = []\n    \n    # Sort the list of filenames\n    file_list = sorted(os.listdir(directory))\n    \n    # Load images from the directory\n    for i, filename in enumerate(file_list):\n        img_path = os.path.join(directory, filename)\n        im = cv2.imread(img_path)\n        im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n        im = np.expand_dims(im, axis =0)\n        images.append(im)\n        if i == 999: break\n\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:47.199826Z","iopub.execute_input":"2023-12-10T19:07:47.200296Z","iopub.status.idle":"2023-12-10T19:07:47.209170Z","shell.execute_reply.started":"2023-12-10T19:07:47.200169Z","shell.execute_reply":"2023-12-10T19:07:47.208038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, x_test, y_test):\n    \"\"\"\n    Evaluate a model using various metrics.\n\n    Parameters:\n    - model: Trained model to be evaluated.\n    - x_test: List of images as numpy arrays.\n    - y_test: List of corresponding labels.\n\n    Returns:\n    None\n    \"\"\"\n    # Generate predictions\n    model_predictions = [np.argmax(model.predict(img)[0]) for img in x_test]\n\n    # Report test accuracy\n    test_accuracy = accuracy_score(y_test, model_predictions)\n    print('Model Evaluation:')\n    print(f'Test accuracy: {test_accuracy:.4f}')\n\n    # Confusion Matrix\n    print('\\nConfusion Matrix:')\n    ConfusionMatrixDisplay.from_predictions(y_test, model_predictions, display_labels=['Men', 'Women'], cmap=plt.cm.Blues)\n\n    # Classification Report\n    print('\\nClassification Report:')\n    print(classification_report(y_test, model_predictions))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:48.131970Z","iopub.execute_input":"2023-12-10T19:07:48.133066Z","iopub.status.idle":"2023-12-10T19:07:48.139771Z","shell.execute_reply.started":"2023-12-10T19:07:48.133027Z","shell.execute_reply":"2023-12-10T19:07:48.138546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Importation & Augmentation\n\n<p>Data augmentation is applied using the `ImageDataGenerator` to enhance the diversity of the training dataset. Data augmentation involves performing various transformations on the existing images, such as rotation, shifting, shearing, zooming, and flipping. These augmented images are then used during training to expose the model to a wider range of variations, helping it generalize better to unseen data. The augmentation process aids in mitigating overfitting and improving the model's ability to handle diverse real-world scenarios by creating a more robust and varied training set.</p>","metadata":{}},{"cell_type":"code","source":"# === Image Data Generators for Training and Validation ===\n\n# Training Data Augmentation:\n# - `rescale=1./255`: Normalize pixel values to the range [0, 1].\n# - `rotation_range=30`: Randomly rotate images by up to 30 degrees.\n# - `width_shift_range=0.2`: Randomly shift images horizontally by up to 20% of the width.\n# - `height_shift_range=0.2`: Randomly shift images vertically by up to 20% of the height.\n# - `shear_range=0.2`: Apply shear transformations with a maximum intensity of 20%.\n# - `zoom_range=0.2`: Randomly zoom into images by up to 20%.\n# - `horizontal_flip=True`: Randomly flip images horizontally.\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\n# Validation Data Preprocessing:\n# - `rescale=1./255`: Normalize pixel values to the range [0, 1].\nvalid_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:51.167643Z","iopub.execute_input":"2023-12-10T19:07:51.168003Z","iopub.status.idle":"2023-12-10T19:07:51.173841Z","shell.execute_reply.started":"2023-12-10T19:07:51.167975Z","shell.execute_reply":"2023-12-10T19:07:51.172349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === Image Data Generators for Training and Validation Datasets ===\n\n# Training Data Generator:\n# - `flow_from_directory`: Generates batches of augmented training data from a directory.\n# - `/kaggle/input/men-women-classification/data`: Path to the training data directory.\n# - `target_size=(IMG_WIDTH, IMG_HEIGHT)`: Resizes images to the specified dimensions.\n# - `class_mode='categorical'`: Uses categorical labels for multi-class classification.\n# - `batch_size=BATCH_SIZE`: Number of samples per batch during training.\n# - `seed=42`: Sets a seed for reproducibility.\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/men-women-classification/data',\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    class_mode='categorical',\n    batch_size=BATCH_SIZE, seed=42)\n\n# Validation Data Generator:\n# - `flow_from_directory`: Generates batches of validation data from a directory.\n# - `/kaggle/input/menwomen-classification/testdata/testdata`: Path to the validation data directory.\n# - `target_size=(IMG_WIDTH, IMG_HEIGHT)`: Resizes images to the specified dimensions.\n# - `class_mode='categorical'`: Uses categorical labels for multi-class classification.\n# - `batch_size=BATCH_SIZE`: Number of samples per batch during validation.\n# - `seed=42`: Sets a seed for reproducibility.\n# - `shuffle=False`: Disables shuffling to maintain order during evaluation.\n\nvalid_generator = valid_datagen.flow_from_directory(\n    '/kaggle/input/menwomen-classification/testdata/testdata',\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    class_mode='categorical',\n    batch_size=BATCH_SIZE, seed=42, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:52.894280Z","iopub.execute_input":"2023-12-10T19:07:52.894681Z","iopub.status.idle":"2023-12-10T19:07:54.499243Z","shell.execute_reply.started":"2023-12-10T19:07:52.894647Z","shell.execute_reply":"2023-12-10T19:07:54.498242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:54.898248Z","iopub.execute_input":"2023-12-10T19:07:54.898672Z","iopub.status.idle":"2023-12-10T19:07:54.905773Z","shell.execute_reply.started":"2023-12-10T19:07:54.898642Z","shell.execute_reply":"2023-12-10T19:07:54.904733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images_subplots('/kaggle/input/men-women-classification/data')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T19:07:55.993753Z","iopub.execute_input":"2023-12-10T19:07:55.994880Z","iopub.status.idle":"2023-12-10T19:07:56.540526Z","shell.execute_reply.started":"2023-12-10T19:07:55.994833Z","shell.execute_reply":"2023-12-10T19:07:56.539303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gender Recognition - Build Model","metadata":{}},{"cell_type":"markdown","source":"#### 1. InceptionV3\nThe Inception-V3 model is a powerful convolutional neural network (CNN) designed for image classification tasks. Its architecture incorporates various convolutional and pooling layers, enabling it to capture intricate patterns and features in images. Below is a simplified visualization of the Inception-V3 model structure:\n![](https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png)\n**Source:** [https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png](https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png)\n\n","metadata":{}},{"cell_type":"code","source":"# Load the InceptionV3 model pre-trained on ImageNet data\ninception_model = InceptionV3(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights='imagenet')\n\n# Build the custom model\nincv3_model = build_custom_model(inception_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T17:53:45.748284Z","iopub.execute_input":"2023-12-01T17:53:45.748551Z","iopub.status.idle":"2023-12-01T17:53:49.676423Z","shell.execute_reply.started":"2023-12-01T17:53:45.748529Z","shell.execute_reply":"2023-12-01T17:53:49.675004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ninc_history = train_model(incv3_model, train_generator, valid_generator, 'inceptionV3_model.h5')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-01T17:53:49.677539Z","iopub.execute_input":"2023-12-01T17:53:49.677829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\nincv3_model.evaluate(valid_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The model after NUM_EPOCHS got an accuracy over the validation data of **95.41%**.","metadata":{}},{"cell_type":"code","source":"x_test, y_test = load_images_and_labels_for_testing()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(incv3_model, x_test, y_test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model architecture\nplot_model(\n    incv3_model,\n    to_file='incv3_model.png',\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=300,\n    show_trainable=True,\n    \n)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\nincv3_model.save('/kaggle/working/inceptionv3.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(inc_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, predictions = get_random_image_with_prediction(incv3_model, '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images_with_predictions(images, predictions, card_width=200, border_radius=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Xception\nThe Xception model, an extension of the Inception architecture, stands out for its depth-wise separable convolutions. Developed to excel in image classification tasks, Xception enhances the efficiency of feature extraction and pattern recognition. Let's delve into the key components of the Xception model:\n![](https://www.researchgate.net/profile/Abid_Mehmood3/publication/355098045/figure/fig2/AS:1076622409109511@1633698193851/Proposed-structure-of-Xception-network-used-within-each-stream-of-CNN.ppm)\n**Source:** [https://www.researchgate.net/profile/Abid_Mehmood3/publication/355098045/figure/fig2/AS:1076622409109511@1633698193851/Proposed-structure-of-Xception-network-used-within-each-stream-of-CNN.ppm](https://www.researchgate.net/profile/Abid_Mehmood3/publication/355098045/figure/fig2/AS:1076622409109511@1633698193851/Proposed-structure-of-Xception-network-used-within-each-stream-of-CNN.ppm)","metadata":{}},{"cell_type":"code","source":"# Load the InceptionV3 model pre-trained on ImageNet data\nxc_model = Xception(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights='imagenet')\n\n# Build the custom model\nxception_model = build_custom_model(xc_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nxc_history = train_model(xception_model, train_generator, valid_generator, save_model='xception.h5')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xception_model.evaluate(valid_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The model after NUM_EPOCHS got an accuracy over the validation data of **98.65%**.","metadata":{}},{"cell_type":"code","source":"evaluate_model(xception_model, x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model architecture\nplot_model(\n    xception_model,\n    to_file='xception_model.png',\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=300,\n    show_trainable=True,\n    \n)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xception_model.save('/kaggle/working/xception.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(xc_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}